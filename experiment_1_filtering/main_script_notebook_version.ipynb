{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main_script import main\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--filter_thv'], dest='filter_thv', nargs=None, const=None, default=None, type=<class 'str'>, choices=None, required=False, help='Threshold values corresponding to the colums specified in --filter_cols (comma separate them!). Will filter out everything below those values.', metavar=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the arguments (copied from main_script.py):\n",
    "parser = argparse.ArgumentParser(\n",
    "    description=    \"A simple script to quickly run different models and \"\n",
    "                    \"different (filtered) versions of the dataset with the \"\n",
    "                    \"exact same hyperparameters, etc.\"\n",
    ")\n",
    "parser.add_argument(\"--experiment\", type=str, required=True,\n",
    "                    help=\"Name of the experiment for saving.\")\n",
    "parser.add_argument(\"--model\", type=str, required=True,\n",
    "                    help=   \"'ðŸ¤— transformers' model name \"\n",
    "                            \"(e.g. 'GroNLP/bert-base-dutch-cased')\")\n",
    "parser.add_argument(\"--train_inp_cols\", type=str, required=True,\n",
    "                    default=\"maximedb/sick_nl\",\n",
    "                    help=\"Columns for train input (comma separate them!)\")\n",
    "parser.add_argument(\"--test_inp_cols\", type=str, required=True,\n",
    "                    default=\"maximedb/sick_nl\",\n",
    "                    help=\"Columns for test input (comma separate them!)\")\n",
    "parser.add_argument(\"--wandb_log\", action=\"store_true\",\n",
    "                    help=\"Log to wandb. Project name is experiment name.\")\n",
    "\n",
    "# Filtering arguments:\n",
    "parser.add_argument(\"--filter_cols\", type=str,\n",
    "                    help=\"The columns in the training dataset to apply a \"\n",
    "                    \"threshold filter to (comma separate them!). \"\n",
    "                    \"IMPORTANT! --filter_thv must contain equally many \"\n",
    "                    \"comma separated threshold values!\")\n",
    "parser.add_argument(\"--filter_thv\", type=str,\n",
    "                    help=\"Threshold values corresponding to the colums \"\n",
    "                    \"specified in --filter_cols (comma separate them!). \"\n",
    "                    \"Will filter out everything below those values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments for different runs:\n",
    "# TODO: change these to the ones you need to run!\n",
    "run_args = [\n",
    "    \"--experiment ik-nlp-mt-quality-filter --model google-bert/bert-base-cased --train_inp_cols premise_en,hypothesis_en --test_inp_cols sentence_A_original,sentence_B_original --wandb_log\",\n",
    "    \n",
    "    # 50% da:\n",
    "    \"--experiment ik-nlp-mt-quality-filter --model GroNLP/bert-base-dutch-cased --train_inp_cols premise_nl,hypothesis_nl --test_inp_cols sentence_A,sentence_B --wandb_log --filter_cols da_premise,da_hypothesis --filter_thv 0.2624,0.2624\",\n",
    "    \"--experiment ik-nlp-mt-quality-filter --model google-bert/bert-base-cased --train_inp_cols premise_en,hypothesis_en --test_inp_cols sentence_A_original,sentence_B_original --wandb_log --filter_cols da_premise,da_hypothesis --filter_thv 0.2624,0.2624\",\n",
    "\n",
    "    # 25% da:\n",
    "    \"--experiment ik-nlp-mt-quality-filter --model GroNLP/bert-base-dutch-cased --train_inp_cols premise_nl,hypothesis_nl --test_inp_cols sentence_A,sentence_B --wandb_log --filter_cols da_premise,da_hypothesis --filter_thv 0.4861,0.4861\",\n",
    "    \"--experiment ik-nlp-mt-quality-filter --model google-bert/bert-base-cased --train_inp_cols premise_en,hypothesis_en --test_inp_cols sentence_A_original,sentence_B_original --wandb_log --filter_cols da_premise,da_hypothesis --filter_thv 0.4861,0.4861\",\n",
    "\n",
    "    # 10% da:\n",
    "    \"--experiment ik-nlp-mt-quality-filter --model GroNLP/bert-base-dutch-cased --train_inp_cols premise_nl,hypothesis_nl --test_inp_cols sentence_A,sentence_B --wandb_log --filter_cols da_premise,da_hypothesis --filter_thv 0.6651,0.6651\",\n",
    "    \"--experiment ik-nlp-mt-quality-filter --model google-bert/bert-base-cased --train_inp_cols premise_en,hypothesis_en --test_inp_cols sentence_A_original,sentence_B_original --wandb_log --filter_cols da_premise,da_hypothesis --filter_thv 0.6651,0.6651\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment with:\n",
      "    model:           google-bert/bert-base-cased\n",
      "    train_cols:      premise_en,hypothesis_en\n",
      "    test_cols:       sentence_A_original,sentence_B_original\n",
      "\n",
      "Getting the training set.\n",
      "Filtering the training dataset.\n",
      "No filters specified. Using full dataset!\n",
      "Tokenizing dataset for training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "706e0284feae491da18ff1252241b40a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vince\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\vince\\.cache\\huggingface\\hub\\models--google-bert--bert-base-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29884ac2459c4b78adc1b2e759b9a236",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f9f6e719a414e6183c4234bbcba9769",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54b34a801dc440348af368f74ccaba48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c865c905d7ef4633b7d92c02e0a7a26e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/549367 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "401fae18226343a69cb4f98c36ce776c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9842 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a096e6f8877144dea22468da9f150d1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9824 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mindooradventurer\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\vince\\Documents\\Natural Language Processing\\IK-NLP24-gr9\\experiment_1_filtering\\wandb\\run-20240327_202816-8aner3r2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/indooradventurer/ik-nlp-mt-quality-filter/runs/8aner3r2' target=\"_blank\">google-bert_bert-base-cased_TRC=premise_en,hypothesis_en_TEC=sentence_A_original,sentence_B_original_</a></strong> to <a href='https://wandb.ai/indooradventurer/ik-nlp-mt-quality-filter' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/indooradventurer/ik-nlp-mt-quality-filter' target=\"_blank\">https://wandb.ai/indooradventurer/ik-nlp-mt-quality-filter</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/indooradventurer/ik-nlp-mt-quality-filter/runs/8aner3r2' target=\"_blank\">https://wandb.ai/indooradventurer/ik-nlp-mt-quality-filter/runs/8aner3r2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model! Using ðŸ¤— defaults, and batch_size=32.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c8ffcada9d7480ab597d16298892434",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ae57022234f44dba4930a8870858575",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6733, 'grad_norm': 5.986367225646973, 'learning_rate': 4.8529411764705885e-05, 'epoch': 0.03}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9f756edced14731a13177cbddb34975",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/308 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ik-nlp-mt-quality-filter\\checkpoint-500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.48820823431015015, 'eval_accuracy': 0.8149766307661045, 'eval_runtime': 18.0983, 'eval_samples_per_second': 543.808, 'eval_steps_per_second': 17.018, 'epoch': 0.03}\n",
      "{'loss': 0.5496, 'grad_norm': 5.231833457946777, 'learning_rate': 4.705882352941177e-05, 'epoch': 0.06}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5af9bbc395e14468a1bb2340e851566e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/308 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ik-nlp-mt-quality-filter\\checkpoint-1000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.44785794615745544, 'eval_accuracy': 0.8286933550091444, 'eval_runtime': 18.1764, 'eval_samples_per_second': 541.47, 'eval_steps_per_second': 16.945, 'epoch': 0.06}\n",
      "{'loss': 0.4943, 'grad_norm': 5.65963888168335, 'learning_rate': 4.558823529411765e-05, 'epoch': 0.09}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13ae7177b5d04f0799013b585d979c45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/308 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ik-nlp-mt-quality-filter\\checkpoint-1500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4352380931377411, 'eval_accuracy': 0.8365169680959155, 'eval_runtime': 18.4011, 'eval_samples_per_second': 534.86, 'eval_steps_per_second': 16.738, 'epoch': 0.09}\n",
      "{'loss': 0.4875, 'grad_norm': 4.497253894805908, 'learning_rate': 4.411764705882353e-05, 'epoch': 0.12}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8df58844c74d41bfa54c4ccf04876250",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/308 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ik-nlp-mt-quality-filter\\checkpoint-2000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4305877089500427, 'eval_accuracy': 0.8404795773216825, 'eval_runtime': 18.1871, 'eval_samples_per_second': 541.153, 'eval_steps_per_second': 16.935, 'epoch': 0.12}\n",
      "{'loss': 0.4592, 'grad_norm': 15.808145523071289, 'learning_rate': 4.2647058823529415e-05, 'epoch': 0.15}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eead7fb430d442758fe1f432624a5b97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/308 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ik-nlp-mt-quality-filter\\checkpoint-2500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4026854634284973, 'eval_accuracy': 0.8539930908351961, 'eval_runtime': 18.2288, 'eval_samples_per_second': 539.914, 'eval_steps_per_second': 16.896, 'epoch': 0.15}\n"
     ]
    }
   ],
   "source": [
    "for run in run_args:\n",
    "    args = parser.parse_args(run.split())\n",
    "    main(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
